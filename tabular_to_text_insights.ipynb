{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083ff27d",
   "metadata": {},
   "source": [
    "\n",
    "# Tabular → Insight Text: E-A-D (Encoder → Aggregator → Decoder) Pipeline\n",
    "\n",
    "This notebook builds a full pipeline to convert tabular data (rows × features) into natural-language **business insights**.\n",
    "\n",
    "**Architecture**  \n",
    "- **E — Encoder (TabTransformer-style):** Produces a per-row embedding from categorical + continuous features.  \n",
    "- **A — Aggregator (Attention Pooling):** Collapses *N* row embeddings into a single **table embedding**.  \n",
    "- **D — Decoder (GPT-2 with Soft Prompting):** Maps the table embedding to a small set of **soft prompt tokens** that condition a decoder-only LLM (GPT-2) to generate insight text.\n",
    "\n",
    "> The notebook uses a **synthetic dataset** where each example is a small table (many rows) plus a templated ground-truth insight. This keeps training fully supervised and easy to iterate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdb5104",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup\n",
    "\n",
    "You'll need a recent PyTorch and Transformers. If you're in Colab or a fresh environment, run the cell below. If you're offline or already installed, you can skip it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a876abf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adria\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# If needed, uncomment:\n",
    "# !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -q transformers accelerate datasets\n",
    "\n",
    "import os, math, random, json, sys, time\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, get_linear_schedule_with_warmup\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "seed = 42\n",
    "random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8685c",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Synthetic Data: Tables + Ground-Truth Insights\n",
    "\n",
    "We synthesize many **mini-tables** (each is a separate training example). For each table, we bake in a couple of clear patterns, then render a short **ground-truth insight** string describing those patterns.  \n",
    "This lets us train end-to-end without needing a curated labeled corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c6cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val sizes: 800 160\n",
      "Sample insight: Customers from Country_0 show elevated churn (18.2% vs 13.3% elsewhere). Average balances are higher in Country_0 ($1,553 vs $1,000). The Enterprise segment als ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Synthetic schema ---\n",
    "CATEG_COLS = [\n",
    "    (\"country\", 12),  # e.g., 12 countries\n",
    "    (\"segment\", 4),   # SMB / Mid / Enterprise / Consumer\n",
    "    (\"product\", 6),   # product lines\n",
    "]\n",
    "NUM_CONT = 6  # numerical features, e.g., balance, age, tenure, etc.\n",
    "\n",
    "# We'll generate a binary column we don't feed to the model directly (churn/fraud)\n",
    "# but we use it to derive target insight text for supervision.\n",
    "# Each table is its own training example.\n",
    "@dataclass\n",
    "class TableConfig:\n",
    "    n_rows_min: int = 80\n",
    "    n_rows_max: int = 150\n",
    "    p_base_churn: float = 0.15\n",
    "\n",
    "def generate_one_table(cfg: TableConfig):\n",
    "    n_rows = random.randint(cfg.n_rows_min, cfg.n_rows_max)\n",
    "    # Choose one \"hot\" country that has higher churn and higher balances on average\n",
    "    hot_country = random.randint(0, CATEG_COLS[0][1] - 1)\n",
    "    hot_segment = random.randint(0, CATEG_COLS[1][1] - 1)\n",
    "\n",
    "    # Continuous feature generative params\n",
    "    # balance ~ higher for hot_country; age, tenure, etc.\n",
    "    base_balance_mu = 1000.0\n",
    "    base_balance_sigma = 400.0\n",
    "    balance_boost = random.uniform(300, 800)  # hot country boost\n",
    "\n",
    "    # Base churn odds + boosts\n",
    "    p_churn_base = cfg.p_base_churn\n",
    "    p_churn_hot_country_boost = random.uniform(0.10, 0.25)\n",
    "    p_churn_hot_segment_boost = random.uniform(0.05, 0.15)\n",
    "\n",
    "    x_categ = torch.zeros(n_rows, len(CATEG_COLS), dtype=torch.long)\n",
    "    x_cont  = torch.zeros(n_rows, NUM_CONT, dtype=torch.float32)\n",
    "    churn   = torch.zeros(n_rows, dtype=torch.long)\n",
    "\n",
    "    for i in range(n_rows):\n",
    "        c_country = random.randint(0, CATEG_COLS[0][1] - 1)\n",
    "        c_segment = random.randint(0, CATEG_COLS[1][1] - 1)\n",
    "        c_product = random.randint(0, CATEG_COLS[2][1] - 1)\n",
    "        x_categ[i] = torch.tensor([c_country, c_segment, c_product])\n",
    "\n",
    "        # Continuous features (toy):\n",
    "        # 0: balance, 1: age, 2: tenure, 3: tx_freq, 4: complaints, 5: income\n",
    "        balance_mu = base_balance_mu + (balance_boost if c_country == hot_country else 0.0)\n",
    "        balance = random.gauss(balance_mu, base_balance_sigma)\n",
    "        age = max(18, random.gauss(42, 12))\n",
    "        tenure = max(0.0, random.gauss(3.5, 2.0))\n",
    "        tx_freq = max(0.0, random.gauss(8, 3))\n",
    "        complaints = max(0.0, random.gauss(0.6, 0.7))\n",
    "        income = max(0.0, random.gauss(48_000, 15_000))\n",
    "\n",
    "        x_cont[i] = torch.tensor([balance, age, tenure, tx_freq, complaints, income], dtype=torch.float32)\n",
    "\n",
    "        # Churn probability with boosts for hot country + hot segment\n",
    "        p = p_churn_base\n",
    "        if c_country == hot_country:\n",
    "            p += p_churn_hot_country_boost\n",
    "        if c_segment == hot_segment:\n",
    "            p += p_churn_hot_segment_boost\n",
    "        churn[i] = 1 if random.random() < p else 0\n",
    "\n",
    "    # Derive summary stats for ground-truth insight\n",
    "    # Country-level churn: compare hot_country vs rest\n",
    "    mask_hot = (x_categ[:,0] == hot_country)\n",
    "    churn_hot = churn[mask_hot].float().mean().item() if mask_hot.any() else 0.0\n",
    "    churn_rest = churn[~mask_hot].float().mean().item() if (~mask_hot).any() else 0.0\n",
    "\n",
    "    avg_balance_hot = x_cont[mask_hot,0].mean().item() if mask_hot.any() else 0.0\n",
    "    avg_balance_rest = x_cont[~mask_hot,0].mean().item() if (~mask_hot).any() else 0.0\n",
    "\n",
    "    # Segment-level churn for the chosen hot segment\n",
    "    mask_seg = (x_categ[:,1] == hot_segment)\n",
    "    churn_seg = churn[mask_seg].float().mean().item() if mask_seg.any() else 0.0\n",
    "    churn_not_seg = churn[~mask_seg].float().mean().item() if (~mask_seg).any() else 0.0\n",
    "\n",
    "    # Build human-readable labels from IDs\n",
    "    def name_country(i): return f\"Country_{i}\"\n",
    "    def name_seg(i): return [\"SMB\",\"Mid\",\"Enterprise\",\"Consumer\"][i] if CATEG_COLS[1][1]==4 and i<4 else f\"Segment_{i}\"\n",
    "\n",
    "    hot_country_name = name_country(hot_country)\n",
    "    hot_segment_name = name_seg(hot_segment)\n",
    "\n",
    "    # Ground-truth insight text (templated)\n",
    "    insight = (\n",
    "        f\"Customers from {hot_country_name} show elevated churn ({churn_hot:.1%} vs {churn_rest:.1%} elsewhere). \"\n",
    "        f\"Average balances are higher in {hot_country_name} (${avg_balance_hot:,.0f} vs ${avg_balance_rest:,.0f}). \"\n",
    "        f\"The {hot_segment_name} segment also churns more ({churn_seg:.1%} vs {churn_not_seg:.1%}). \"\n",
    "        f\"Prioritize retention for {hot_country_name} and {hot_segment_name} customers with higher balances.\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"x_categ\": x_categ,\n",
    "        \"x_cont\": x_cont,\n",
    "        \"row_mask\": torch.ones(n_rows, dtype=torch.bool),  # all valid rows\n",
    "        \"insight\": insight,\n",
    "    }\n",
    "\n",
    "class TableToTextDataset(Dataset):\n",
    "    def __init__(self, n_tables=800, cfg: TableConfig=TableConfig()):\n",
    "        self.samples = [generate_one_table(cfg) for _ in range(n_tables)]\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "train_ds = TableToTextDataset(n_tables=800)\n",
    "val_ds   = TableToTextDataset(n_tables=160)\n",
    "print(\"Train/Val sizes:\", len(train_ds), len(val_ds))\n",
    "print(\"Sample insight:\", train_ds[0][\"insight\"][:160], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a1f9b9",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Tokenizer & DataLoader (variable rows per table)\n",
    "\n",
    "- We pad **rows** per table within a batch and carry a boolean `row_mask` for attention pooling.  \n",
    "- We tokenize the target insight text and pad to the max text length in the batch.  \n",
    "- We set `pad_token = eos_token` for GPT-2 to simplify handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a515e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes:\n",
      "  x_categ (4, 133, 3)\n",
      "  x_cont (4, 133, 6)\n",
      "  row_mask (4, 133)\n",
      "  input_ids (4, 74)\n",
      "  attention_mask_text (4, 74)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# GPT-2 tokenizer\n",
    "tokenizer_name = \"gpt2\"  # you can switch to \"distilgpt2\" for a smaller model\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_TEXT_TOKENS = 160  # cap for training (keep moderate)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Compute max rows & pad\n",
    "    max_rows = max(b[\"x_categ\"].shape[0] for b in batch)\n",
    "    B = len(batch)\n",
    "    Cc = len(CATEG_COLS)\n",
    "    Nc = NUM_CONT\n",
    "\n",
    "    pad_categ = torch.zeros(B, max_rows, Cc, dtype=torch.long)\n",
    "    pad_cont  = torch.zeros(B, max_rows, Nc, dtype=torch.float32)\n",
    "    row_mask  = torch.zeros(B, max_rows, dtype=torch.bool)\n",
    "\n",
    "    insights = [b[\"insight\"] for b in batch]\n",
    "    for i, b in enumerate(batch):\n",
    "        n = b[\"x_categ\"].shape[0]\n",
    "        pad_categ[i, :n] = b[\"x_categ\"]\n",
    "        pad_cont[i, :n]  = b[\"x_cont\"]\n",
    "        row_mask[i, :n]  = True\n",
    "\n",
    "    # Tokenize and pad targets\n",
    "    tok = tokenizer(\n",
    "        insights,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_TEXT_TOKENS,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = tok[\"input_ids\"]\n",
    "    attention_mask_text = tok[\"attention_mask\"]  # 1 for real tokens, 0 for pads\n",
    "\n",
    "    return {\n",
    "        \"x_categ\": pad_categ,\n",
    "        \"x_cont\": pad_cont,\n",
    "        \"row_mask\": row_mask,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask_text\": attention_mask_text,\n",
    "    }\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch shapes:\")\n",
    "for k,v in batch.items():\n",
    "    print(\" \", k, tuple(v.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f6166",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Model — E: Row Encoder (TabTransformer-style)\n",
    "\n",
    "- Each categorical column has its **own embedding**.  \n",
    "- We treat a row's categorical values as a **token sequence** and pass them through a small Transformer encoder.  \n",
    "- Continuous features go through an MLP and are concatenated with the categorical representation.  \n",
    "- Finally projected to a compact **row embedding**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8f6ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CatEmbeddingBlock(nn.Module):\n",
    "    def __init__(self, cardinalities: List[int], d_cat: int):\n",
    "        super().__init__()\n",
    "        self.embeds = nn.ModuleList([nn.Embedding(n, d_cat) for (_, n) in CATEG_COLS])\n",
    "        for emb in self.embeds:\n",
    "            nn.init.normal_(emb.weight, std=0.02)\n",
    "\n",
    "    def forward(self, x_categ):  # [B, R, Cc]\n",
    "        B,R,Cc = x_categ.shape\n",
    "        embs = []\n",
    "        for j in range(Cc):\n",
    "            embs.append(self.embeds[j](x_categ[:,:,j]))  # [B, R, d_cat]\n",
    "        # Stack into [B, R, Cc, d_cat] then view as sequence over Cc per row\n",
    "        cat_tok = torch.stack(embs, dim=2)  # [B, R, Cc, d_cat]\n",
    "        return cat_tok\n",
    "\n",
    "class RowEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_cat: int = 32,\n",
    "                 n_heads: int = 4,\n",
    "                 n_layers: int = 2,\n",
    "                 d_ff: int = 128,\n",
    "                 d_cont: int = 64,\n",
    "                 row_emb_dim: int = 128,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.Cc = len(CATEG_COLS)\n",
    "        self.Nc = NUM_CONT\n",
    "        self.d_cat = d_cat\n",
    "        self.cat_block = CatEmbeddingBlock([n for _,n in CATEG_COLS], d_cat)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_cat, nhead=n_heads, dim_feedforward=d_ff,\n",
    "            dropout=dropout, batch_first=True, activation=\"gelu\"\n",
    "        )\n",
    "        self.cat_encoder = nn.TransformerEncoder(enc_layer, num_layers=n_layers)\n",
    "        self.cat_ln = nn.LayerNorm(d_cat)\n",
    "\n",
    "        self.cont_mlp = nn.Sequential(\n",
    "            nn.LayerNorm(self.Nc),\n",
    "            nn.Linear(self.Nc, d_cont),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_cont, d_cont),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Linear(d_cat + d_cont, row_emb_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(row_emb_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_categ, x_cont):  # x_categ:[B,R,Cc], x_cont:[B,R,Nc]\n",
    "        B,R,Cc = x_categ.shape\n",
    "        cat_tok = self.cat_block(x_categ)  # [B, R, Cc, d_cat]\n",
    "        # Process cat sequence per row: we reshape to merge batch and rows\n",
    "        cat_tok = cat_tok.view(B*R, Cc, self.d_cat)  # [B*R, Cc, d_cat]\n",
    "        cat_enc = self.cat_encoder(cat_tok)          # [B*R, Cc, d_cat]\n",
    "        cat_enc = self.cat_ln(cat_enc.mean(dim=1))   # [B*R, d_cat] mean over Cc tokens\n",
    "\n",
    "        cont_enc = self.cont_mlp(x_cont.view(B*R, -1))  # [B*R, d_cont]\n",
    "\n",
    "        row = torch.cat([cat_enc, cont_enc], dim=-1)    # [B*R, d_cat+d_cont]\n",
    "        row = self.fuse(row)                            # [B*R, row_emb_dim]\n",
    "        row = row.view(B, R, -1)                        # [B, R, row_emb_dim]\n",
    "        return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2664e",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Model — A: Attention Pooling over Rows\n",
    "\n",
    "A simple attention mechanism over rows to produce a fixed-size **table embedding**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6260d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, dim, hidden=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.att = nn.Sequential(\n",
    "            nn.Linear(dim, hidden), nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, row_mask=None):  # x:[B,R,D]\n",
    "        scores = self.att(x).squeeze(-1)  # [B,R]\n",
    "        if row_mask is not None:\n",
    "            scores = scores.masked_fill(~row_mask, float('-inf'))\n",
    "        w = torch.softmax(scores, dim=1)  # [B,R]\n",
    "        pooled = torch.bmm(w.unsqueeze(1), x).squeeze(1)  # [B,D]\n",
    "        return pooled, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4785f392",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Model — D: GPT-2 with Soft Prompting\n",
    "\n",
    "We map the table embedding into **K learnable soft tokens** in GPT-2's hidden space.  \n",
    "During training we concatenate these soft tokens with the token embeddings of the target text and train with standard next-token LM loss (labels = text tokens, soft tokens masked with `-100`).  \n",
    "During generation we feed only the soft tokens and let GPT-2 produce text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be912b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SoftPromptProjector(nn.Module):\n",
    "    def __init__(self, in_dim, gpt_hidden, K=20, hidden=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.out_dim = gpt_hidden\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, K * gpt_hidden)\n",
    "        )\n",
    "\n",
    "    def forward(self, table_emb):  # [B, in_dim]\n",
    "        B = table_emb.size(0)\n",
    "        x = self.net(table_emb)                # [B, K*H]\n",
    "        x = x.view(B, self.K, self.out_dim)    # [B, K, H]\n",
    "        return x\n",
    "\n",
    "class InsightDecoder(nn.Module):\n",
    "    def __init__(self, model_name=\"gpt2\", freeze_gpt2=False):\n",
    "        super().__init__()\n",
    "        self.gpt2 = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.hidden = self.gpt2.config.n_embd\n",
    "        if freeze_gpt2:\n",
    "            for p in self.gpt2.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    def forward_train(self, soft_embeds, input_ids, attention_mask_text):\n",
    "        # soft_embeds: [B, K, H]\n",
    "        # input_ids: [B, T], attention_mask_text: [B, T] (1 for real tokens)\n",
    "        B, K, H = soft_embeds.shape\n",
    "        T = input_ids.size(1)\n",
    "\n",
    "        # Get word embeddings for tokens\n",
    "        tok_embeds = self.gpt2.transformer.wte(input_ids)  # [B, T, H]\n",
    "\n",
    "        inputs_embeds = torch.cat([soft_embeds, tok_embeds], dim=1)  # [B, K+T, H]\n",
    "\n",
    "        # Build attention mask: soft tokens are all visible (1s), then text mask\n",
    "        attn_mask = torch.cat([torch.ones(B, K, dtype=attention_mask_text.dtype, device=input_ids.device),\n",
    "                               attention_mask_text], dim=1)  # [B, K+T]\n",
    "\n",
    "        # Labels: -100 for soft tokens, then actual token IDs\n",
    "        labels = torch.cat([torch.full((B, K), -100, dtype=torch.long, device=input_ids.device),\n",
    "                            input_ids], dim=1)  # [B, K+T]\n",
    "\n",
    "        out = self.gpt2(inputs_embeds=inputs_embeds, attention_mask=attn_mask, labels=labels)\n",
    "        return out  # .loss, .logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate_from_soft(self, soft_embeds, max_new_tokens=100, do_sample=True, temperature=0.8, top_p=0.95):\n",
    "        B, K, H = soft_embeds.shape\n",
    "        # Start generation with only the soft prompt\n",
    "        attn_mask = torch.ones(B, K, dtype=torch.long, device=soft_embeds.device)\n",
    "        gen_ids = self.gpt2.generate(\n",
    "            inputs_embeds=soft_embeds,\n",
    "            attention_mask=attn_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        # gen_ids will be token IDs *for the generated continuation only* in some HF versions.\n",
    "        # To be safe, decode last max_new_tokens tokens.\n",
    "        text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c83bd4",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Full Model Wiring\n",
    "\n",
    "`Table2TextModel` wraps the encoder, aggregator, projector, and decoder, exposing `forward()` for training and a `generate()` utility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a3580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Table2TextModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 row_emb_dim=128,\n",
    "                 gpt_model_name=\"gpt2\",\n",
    "                 freeze_gpt2=True,\n",
    "                 soft_K=20):\n",
    "        super().__init__()\n",
    "        self.encoder = RowEncoder(row_emb_dim=row_emb_dim)\n",
    "        self.aggregator = AttentionPooling(dim=row_emb_dim)\n",
    "        # We instantiate decoder first to know hidden size\n",
    "        self.decoder = InsightDecoder(model_name=gpt_model_name, freeze_gpt2=freeze_gpt2)\n",
    "        gpt_hidden = self.decoder.hidden\n",
    "        self.projector = SoftPromptProjector(in_dim=row_emb_dim, gpt_hidden=gpt_hidden, K=soft_K)\n",
    "\n",
    "    def forward(self, x_categ, x_cont, row_mask, input_ids, attention_mask_text):\n",
    "        # E\n",
    "        row_embs = self.encoder(x_categ, x_cont)  # [B,R,D]\n",
    "        # A\n",
    "        table_emb, _ = self.aggregator(row_embs, row_mask=row_mask)  # [B,D]\n",
    "        # Soft prompt\n",
    "        soft_embeds = self.projector(table_emb)  # [B,K,H]\n",
    "        # D\n",
    "        out = self.decoder.forward_train(soft_embeds, input_ids, attention_mask_text)\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, x_categ, x_cont, row_mask, max_new_tokens=100, **gen_kw):\n",
    "        row_embs = self.encoder(x_categ, x_cont)\n",
    "        table_emb, _ = self.aggregator(row_embs, row_mask=row_mask)\n",
    "        soft = self.projector(table_emb)\n",
    "        texts = self.decoder.generate_from_soft(soft, max_new_tokens=max_new_tokens, **gen_kw)\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd920c9f",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Training\n",
    "\n",
    "We keep GPT-2 **frozen** by default and only train:\n",
    "- Row encoder (TabTransformer-style)\n",
    "- Attention pooling aggregator\n",
    "- Soft prompt projector\n",
    "\n",
    "> You can unfreeze GPT-2 later for better quality if you have GPU memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0c984a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter   20/200 | loss 5.004\n",
      "iter   40/200 | loss 4.170\n",
      "iter   60/200 | loss 3.500\n",
      "iter   80/200 | loss 3.019\n",
      "iter  100/200 | loss 2.383\n",
      "iter  120/200 | loss 1.816\n",
      "iter  140/200 | loss 1.577\n",
      "iter  160/200 | loss 1.254\n",
      "iter  180/200 | loss 1.205\n",
      "iter  200/200 | loss 1.269\n",
      "Epoch 1: train 2.740 | val 0.765  (736.7s)\n",
      "  Saved best checkpoint\n",
      "iter   20/200 | loss 1.072\n",
      "iter   40/200 | loss 1.086\n",
      "iter   60/200 | loss 1.031\n",
      "iter   80/200 | loss 0.923\n",
      "iter  100/200 | loss 1.025\n",
      "iter  120/200 | loss 1.030\n",
      "iter  140/200 | loss 0.859\n",
      "iter  160/200 | loss 0.889\n",
      "iter  180/200 | loss 0.848\n",
      "iter  200/200 | loss 0.848\n",
      "Epoch 2: train 0.976 | val 0.622  (734.5s)\n",
      "  Saved best checkpoint\n",
      "iter   20/200 | loss 0.820\n",
      "iter   40/200 | loss 0.895\n",
      "iter   60/200 | loss 0.916\n",
      "iter   80/200 | loss 0.796\n",
      "iter  100/200 | loss 0.857\n",
      "iter  120/200 | loss 0.803\n",
      "iter  140/200 | loss 0.803\n",
      "iter  160/200 | loss 0.760\n",
      "iter  180/200 | loss 0.817\n",
      "iter  200/200 | loss 0.729\n",
      "Epoch 3: train 0.827 | val 0.607  (910.3s)\n",
      "  Saved best checkpoint\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "EPOCHS = 3\n",
    "GRAD_CLIP = 1.0\n",
    "WARMUP_STEPS = 100\n",
    "LOG_EVERY = 20\n",
    "\n",
    "model = Table2TextModel(gpt_model_name=tokenizer_name, freeze_gpt2=True, soft_K=20).to(device)\n",
    "\n",
    "# Only train unfrozen params\n",
    "optim = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=LEARNING_RATE, weight_decay=0.01)\n",
    "\n",
    "# Simple linear warmup schedule\n",
    "num_training_steps = EPOCHS * math.ceil(len(train_loader))\n",
    "sched = get_linear_schedule_with_warmup(optim, num_warmup_steps=WARMUP_STEPS, num_training_steps=num_training_steps)\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total, n = 0.0, 0\n",
    "    for it, batch in enumerate(loader):\n",
    "        x_categ = batch[\"x_categ\"].to(device)\n",
    "        x_cont  = batch[\"x_cont\"].to(device)\n",
    "        row_mask= batch[\"row_mask\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask_text = batch[\"attention_mask_text\"].to(device)\n",
    "\n",
    "        out = model(x_categ, x_cont, row_mask, input_ids, attention_mask_text)\n",
    "        loss = out.loss\n",
    "\n",
    "        if train:\n",
    "            optim.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            optim.step()\n",
    "            sched.step()\n",
    "\n",
    "        total += loss.item() * x_categ.size(0)\n",
    "        n += x_categ.size(0)\n",
    "\n",
    "        if train and (it+1) % LOG_EVERY == 0:\n",
    "            print(f\"iter {it+1:4d}/{len(loader)} | loss {loss.item():.3f}\")\n",
    "    return total / max(1,n)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = run_epoch(train_loader, train=True)\n",
    "    val_loss = run_epoch(val_loader, train=False)\n",
    "    dt = time.time()-t0\n",
    "    print(f\"Epoch {ep}: train {tr_loss:.3f} | val {val_loss:.3f}  ({dt:.1f}s)\")\n",
    "    if val_loss < best_val:\n",
    "        best_val = val_loss\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        torch.save(model.state_dict(), \"checkpoints/tab2text_best.pt\")\n",
    "        print(\"  Saved best checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ffb9c7",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Inference / Generation Demo\n",
    "\n",
    "We take a validation table, encode → aggregate → project to soft tokens, and ask GPT-2 to **generate** an insight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d2086d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint.\n",
      "---- Generated insight #0 ----\n",
      "Customers from Country_5 show elevated churn (43.7% vs 19.1% elsewhere). Average balances are higher in Country_5 ($2,831 vs $1,822). The SMB segment also churns more (25.1% vs 10.9%). Prioritize retention for Country_5 and SMB customers with higher balances.\n",
      "\n",
      "---- Generated insight #1 ----\n",
      "Customers from Country_4 show elevated churn (27.3% vs 14.9% elsewhere). Average balances are higher in Country_4 ($6,739 vs $5,914). The Enterprise segment also churns more (20.9% vs 15.5%). Prioritize retention for Country_4 and Enterprise customers with higher balances.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load best checkpoint (optional if continuing same session)\n",
    "if os.path.exists(\"checkpoints/tab2text_best.pt\"):\n",
    "    _ = model.load_state_dict(torch.load(\"checkpoints/tab2text_best.pt\", map_location=device), strict=False)\n",
    "    print(\"Loaded best checkpoint.\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(val_loader))\n",
    "    x_categ = batch[\"x_categ\"][:2].to(device)\n",
    "    x_cont  = batch[\"x_cont\"][:2].to(device)\n",
    "    row_mask= batch[\"row_mask\"][:2].to(device)\n",
    "\n",
    "    gen_texts = model.generate(x_categ, x_cont, row_mask, max_new_tokens=120, do_sample=True, temperature=0.8, top_p=0.95)\n",
    "    for i, t in enumerate(gen_texts):\n",
    "        print(f\"---- Generated insight #{i} ----\\n{t}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b8487b",
   "metadata": {},
   "source": [
    "\n",
    "## 10. (Optional) Unfreeze GPT-2 for End-to-End Fine-tuning\n",
    "\n",
    "If you have the VRAM, you can unfreeze GPT-2 to improve quality. It’s slower and heavier, but can help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed07877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# To unfreeze GPT-2, re-initialize model with freeze_gpt2=False, then re-run training.\n",
    "# model = Table2TextModel(gpt_model_name=tokenizer_name, freeze_gpt2=False, soft_K=20).to(device)\n",
    "# optim = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-5, weight_decay=0.01)\n",
    "# ... re-run the training loop with smaller LR for stability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0929ae",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Next Steps & Extensions\n",
    "\n",
    "- **Richer Encoders:** Try feature-wise masking, residual connections across row/feature blocks, or pretrained tabular encoders.  \n",
    "- **Better Aggregation:** Multi-head attention pooling, Set Transformers, or learn multiple table tokens (a *set* of soft prompts).  \n",
    "- **Multiple Insight Types:** Train with multi-target texts (e.g., churn, fraud, revenue) using task tokens.  \n",
    "- **Scaling:** Use `distilgpt2` or larger GPT-2 variants; add gradient checkpointing; train on real tables with curated insight texts.  \n",
    "- **Evaluation:** ROUGE/BLEU for text, plus *factuality checks* comparing generated statements to table stats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e657ad",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Environment Info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7881c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cpu\n",
      "transformers: 4.55.2\n",
      "python: 3.13.2 (tags/v3.13.2:4f8bb39, Feb  4 2025, 15:23:48) [MSC v.1942 64 bit (AMD64)]\n",
      "platform: Windows-11-10.0.26100-SP0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch, transformers, platform\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"python:\", sys.version)\n",
    "print(\"platform:\", platform.platform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35806b13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
